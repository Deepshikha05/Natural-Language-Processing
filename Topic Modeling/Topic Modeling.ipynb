{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "The process of learning, recognizing and extracting these topics across a collection of documents is called Topic Modeling\n",
    "\n",
    "The algorithms that can be used to achieve it are LSA, pLSA, LDA, Idea2Vec\n",
    "\n",
    "All Topic Modeling algorithms runs on following asssumptions:\n",
    "1. Each document consists of a mixture of topics. \n",
    "2. Each topic consists of a collection of words.\n",
    "\n",
    "Topic modeling models are built around the idea that the semantics of our document are actually being governed by some hidden or latent variables that we are not observing. As a result, the goal of topic modeling is to uncover these latent variables - topics - that shape meaning of document and corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA - Latent Semantic Analysis\n",
    "\n",
    "1. It is the foundational technique in topic modeling. \n",
    "2. The idea is to take a matrix of documents and terms and decompose it into a separate document-topic matrix and a topic-term matrix. \n",
    "\n",
    "First step is to generalize the document-term matrix. \n",
    "Given m documents and n words, construct mXn matrix. In which each row represents a document and each column represents a word. \n",
    "\n",
    "In simplest version of LSA, each entry can simply be a raw count of number of times jth word appeared in the ith document. In practice, raw counts do not perform well because they do not account for significance of each word in document. \n",
    "\n",
    "LSA models typically replace raw counts in document-term matrix with tf-ide score.\n",
    "\n",
    "Tf-Idf -> Term Frequency-Inverse Document Frequency assigns a weight for term j in document:\n",
    "\n",
    "$$ w_{i, j} = tf_{i,j} * log(N/df_{j})$$\n",
    "\n",
    "where $$ w_{i, j} $$ is tf-idf score, $$ tf_{i,j} $$ is occurence of term in document and N is total number of docments and $$ df_{j} $$ is documents containing word. \n",
    "\n",
    "- A term has a large weight when it occurs frequently across document but infrequently across corpus. \n",
    "Example - The word \"build\" might appear often in document but because it's likely fairly common in rest of corpus, it will not have high tf-idf score . If word \"genetrification\" appears often in a document because it is rarer in rest of corpus, it will have a higher tf-idf score. \n",
    "\n",
    "Once we have document-term matrix A, we can start thinking about our latent topics. In all likelihood, A is very sparse, very noisy and very redundant across its many dimensions. As a result, to find new latent topics that can capture relationships among words and documents, we want to perform dimensionality reduction on A. \n",
    "\n",
    "Dimensionality reduction can be performed using truncated SVD -> Singular Value Decomposition is a technique in linear algebra that factorizes any matrix M into product of 2 separate matrices \n",
    "$$ M = U * S * V $$\n",
    "s -> Diagonal matrix of singular values of M.\n",
    "\n",
    "Critically truncated SVD reduces dimensionality by reducing selecting only t largest singular values and only keeping first t columns of U * V. In this case, t, is a hyperparameter we can select and adjust to reflect the number of topics we want to find.\n",
    "\n",
    "$$ A = U_{t} * S_{t} * V_{t} $$\n",
    "\n",
    "U -> Document-topic matrix\n",
    "V -> Term-Topic matrix\n",
    "\n",
    "In both U and V, columns correspond to one of the t topics.  In U rows represent document vectors expressed in terms of topics. In V, rows represent document vectors expressed in terms of topics. With these document vectors and term vector, we can now easily apply measures such as cosine similarity to evaluate:\n",
    "1. The similarity of different documents. \n",
    "2. The similarity of different words. \n",
    "3. The similarity of terms (or queries) and documents (which becomes useful in information retrieval when we want to retrieve passages most relevant to our search query). \n",
    "\n",
    "LSA Drawbacks:\n",
    "1. Lack of interpretable embeddings (we do not know what topics are and components may be arbitrarily positive or negative). \n",
    "2. Need for really large set of documents and vocabulary to get accurate results. \n",
    "3. Less efficient representations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pLSA - Provbabilistic Latent Semantic Analysis\n",
    "\n",
    "1. Uses probabilistic model instead of SVD to tackle problem. \n",
    "2. The core idea is to find a probabilistic model with latent topics that can generate data we observe in document-term matric. \n",
    "\n",
    "Basic Assumptions:\n",
    "1. Each document consists of a mixture of topics. \n",
    "2. Each topic consists of a collection of words. \n",
    "\n",
    "pLSA adds a probabilistic spin to these assumptions: \n",
    "1. Given a document d, topic z is present in that document with probability P(z/d). \n",
    "2. Given a topic z, word w is drawn from z with probability P(w/z).\n",
    "\n",
    "Joint Probability of seeing a given document and word together is:\n",
    "\n",
    "\n",
    "\n",
    "In this case P(D), P(Z/D), P(W/Z) are parameters of our model. P(D) can be determind directly from our corpus. P(Z/D) and P(W/D) are modeled as multinomial distributions and then can be trained using expectation-minimization algorithm. EM is method of finding the likeliest parameter estimates for a model which depends on unobserved latent variables (in our case - topics). \n",
    "\n",
    "P(D/W) can be equivalently parameterized using a different set of 3 parameters. We can understand this equivalency by looking at the model as a generative process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
