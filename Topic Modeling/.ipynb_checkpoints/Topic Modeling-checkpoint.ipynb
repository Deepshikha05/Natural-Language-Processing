{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "The process of learning, recognizing and extracting these topics across a collection of documents is called Topic Modeling\n",
    "\n",
    "The algorithms that can be used to achieve it are LSA, pLSA, LDA, Idea2Vec\n",
    "\n",
    "All Topic Modeling algorithms runs on following asssumptions:\n",
    "1. Each document consists of a mixture of topics. \n",
    "2. Each topic consists of a collection of words.\n",
    "\n",
    "Topic modeling models are built around the idea that the semantics of our document are actually being governed by some hidden or latent variables that we are not observing. As a result, the goal of topic modeling is to uncover these latent variables - topics - that shape meaning of document and corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA - Latent Semantic Analysis\n",
    "\n",
    "1. It is the foundational technique in topic modeling. \n",
    "2. The idea is to take a matrix of documents and terms and decompose it into a separate document-topic matrix and a topic-term matrix. \n",
    "\n",
    "First step is to generalize the document-term matrix. \n",
    "Given m documents and n words, construct mXn matrix. In which each row represents a document and each column represents a word. \n",
    "\n",
    "In simplest version of LSA, each entry can simply be a raw count of number of times jth word appeared in the ith document. In practice, raw counts do not perform well because they do not account for significance of each word in document. \n",
    "\n",
    "LSA models typically replace raw counts in document-term matrix with tf-ide score.\n",
    "\n",
    "Tf-Idf -> Term Frequency-Inverse Document Frequency assigns a weight for term j in document:\n",
    "\n",
    "$$ w_{i, j} = tf_{i,j} * log(N/df_{j})$$\n",
    "\n",
    "where $$ w_{i, j} $$ is tf-idf score, $$ tf_{i,j} $$ is occurence of term in document and N is total number of docments and $$ df_{j} $$ is documents containing word. \n",
    "\n",
    "- A term has a large weight when it occurs frequently across document but infrequently across corpus. \n",
    "Example - The word \"build\" might appear often in document but because it's likely fairly common in rest of corpus, it will not have high tf-idf score . If word \"genetrification\" appears often in a document because it is rarer in rest of corpus, it will have a higher tf-idf score. \n",
    "\n",
    "Once we have document-term matrix A, we can start thinking about our latent topics. In all likelihood, A is very sparse, very noisy and very redundant across its many dimensions. As a result, to find new latent topics that can capture relationships among words and documents, we want to perform dimensionality reduction on A. \n",
    "\n",
    "Dimensionality reduction can be performed using truncated SVD -> Singular Value Decomposition is a technique in linear algebra that factorizes any matrix M into product of 2 separate matrices \n",
    "$$ M = U * S * V $$\n",
    "s -> Diagonal matrix of singular values of M.\n",
    "\n",
    "Critically truncated SVD reduces dimensionality by reducing selecting only t largest singular values and only keeping first t columns of U * V. In this case, t, is a hyperparameter we can select and adjust to reflect the number of topics we want to find.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
